{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5--Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data science: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Derivation\n",
    "\n",
    "#### Linear formulation\n",
    "\n",
    "$$\\mathcal L=\\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
    "$$\\mathcal L=\\prod_{i=1}^n F(x_i'\\beta)^{y_i}(1-F(x_i'\\beta))^{1-y_i}$$\n",
    "\n",
    "$$\\ln\\mathcal L=\\sum_{i=1}^n y_i \\ln{F(x_i'\\beta)}+(1-y_i)\\ln{(1-F(x_i'\\beta))}$$\n",
    "$$\\ln\\mathcal L=\\left[\\ln{F(\\beta'\\mathbf X')}\\right]y+\\left[\\ln{(\\mathbf{1}'-F(\\beta'\\mathbf X'))}\\right](1-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "expit = lambda x: 1/(1+np.exp(-x))\n",
    "def loglike(x,y,b):\n",
    "    Fx = expit(b.T@x.T)\n",
    "    return np.log(Fx)@y+np.log(1-Fx)@(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)}{F(\\mathbf X\\beta)}\\right)y-\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)}{\\mathbf 1-F(\\mathbf X\\beta)}\\right)(1-y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)(1-F(\\mathbf X\\beta))}{(1-F(\\mathbf X\\beta))F(\\mathbf X\\beta)}\\right)y-\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)F(\\mathbf X\\beta)}{\\mathbf F(\\mathbf X\\beta)(1-F(\\mathbf X\\beta))}\\right)(1-y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(1-F(\\mathbf X\\beta)\\right)y-\\text{diag}\\left(F(\\mathbf X\\beta)\\right)(1-y)\\right]$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(y-F(\\mathbf X\\beta)y-F(\\mathbf X\\beta)+F(\\mathbf X\\beta)y)\\right)\\right]\\mathbf 1$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(y-F(\\mathbf X\\beta)\\right)\\right]\\mathbf 1$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[y-F(\\mathbf X\\beta)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y,b):\n",
    "    Fx = expit(x@b)\n",
    "    return x.T@(y-Fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian\n",
    "$$\\frac{d}{d\\beta}\\frac{d\\ln\\mathcal L}{d\\beta}'=\\frac{d}{d\\beta}\\left[y'-F(\\beta'\\mathbf X')\\right]\\mathbf X$$\n",
    "$$\\frac{d^2\\ln\\mathcal L}{d\\beta d\\beta'}=-\\mathbf X'\\left[\\text{diag}\\left(f(\\mathbf X\\beta)\\right)\\right]\\mathbf X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x,y,b):\n",
    "    Fx = expit(x@b)\n",
    "    fx = Fx*(1-Fx)\n",
    "    return -x.T@np.diagflat(fx.flatten())@x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crammer-Rao Lower Bound\n",
    "\n",
    "**Lemma**\n",
    "\n",
    "Under regularity conditions:\n",
    "\n",
    "(A)\n",
    "$$\\text{E}\\left[\\hat\\theta\\right]=\\theta$$\n",
    "(B)\n",
    "$$\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\right]=0$$\n",
    "(C)\n",
    "$$\\text{E}\\left[\\frac{d^2\\ln{\\mathcal{L}}}{d\\theta d\\theta'}\\right]=-\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]$$\n",
    "(D)\n",
    "$$\\int_\\Omega{\\frac{d\\mathcal{L}}{d\\theta}\\hat\\theta'dz}=\\mathbf{I}_r$$\n",
    "The variance of $\\hat\\theta$ can be bounded by\n",
    "$$\\text{Var}\\left[\\hat\\theta\\right]\\ge\\left[-\\text{E}\\left[\\frac{d^2\\ln{\\mathcal{L}}}{d\\theta d\\theta'}\\right]\\right]^{-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Proof*\n",
    "\n",
    "Define $\\mathbf{P}=\\text{E}\\left[(\\hat\\theta-\\theta)(\\hat\\theta-\\theta)'\\right]$, $\\mathbf{R}=\\text{E}\\left[\\left(d\\ln{\\mathcal{L}}\\big/d\\theta\\right)\\left(d\\ln{\\mathcal{L}}\\big/d\\theta'\\right)\\right]$, and $\\mathbf{Q}=\\text{E}\\left[(\\hat\\theta-\\theta)\\left(d\\ln{\\mathcal{L}}\\big/d\\theta'\\right)\\right]$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{Q}&=\\text{E}\\left[(\\hat\\theta-\\theta)\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right] \\\\\n",
    "          &=\\text{E}\\left[\\hat\\theta\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]+\\theta\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right] \\\\\n",
    "          &=\\text{E}\\left[\\hat\\theta\\frac{1}{\\mathcal{L}}\\frac{d\\mathcal{L}}{d\\theta'}\\right]+\\mathbf{0} \\\\\n",
    "          &=\\int_\\Omega{\\left[\\hat\\theta\\frac{1}{\\mathcal{L}}\\frac{d\\mathcal{L}}{d\\theta'}\\right]\\mathcal{L}dz} \\\\\n",
    "          &=\\int_\\Omega{\\frac{d\\mathcal{L}}{d\\theta}\\hat\\theta' dz} \\\\\n",
    "          &=\\mathbf{I}_r\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{P} & \\mathbf Q \\\\\n",
    "\\mathbf Q' & \\mathbf R \n",
    "\\end{bmatrix}\\ge \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Because the left hand side is a variance-covariance matrix. Premultiply both sides of the equation by $[\\mathbf I_r,\\mathbf R^{-1}]$ and postmultiply by $[\\mathbf I_r,\\mathbf R^{-1}]'$ and you get:\n",
    "\n",
    "$$\\mathbf P - \\mathbf R^{-1} \\ge \\mathbf{0}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\mathbf P \\ge \\mathbf R^{-1}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\text{E}\\left[(\\hat\\theta-\\theta)(\\hat\\theta-\\theta)'\\right] \\ge \\left[\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]\\right]^{-1} $$\n",
    "\n",
    "This can be rewritten as the Crammer-Rao lower bound by substituting the variance definition and assumption (C):\n",
    "\n",
    "$$\\text{Var}\\left[\\hat\\theta\\right]\\ge\\left[-\\text{E}\\left[\\frac{d^2\\ln{\\mathcal{L}}}{d\\theta d\\theta'}\\right]\\right]^{-1}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** Crammer-Rao Lower Bound\n",
    "\n",
    "Assume\n",
    "$\\mathcal{L}$ is continuous and differentiable. For any unbiased estimator $\\hat\\theta$, the variance is bounded below by\n",
    "$$\\text{Var}\\left[\\hat\\theta\\right]\\ge\\left[-\\text{E}\\left[\\frac{d^2\\ln{\\mathcal{L}}}{d\\theta d\\theta'}\\right]\\right]^{-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proof*\n",
    "\n",
    "(A) is assumed because the estimator is unbiased. (B) and (C) true by the fundamental theorem of calculus. D is true by the fundamental theorem of calculus if (A) is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\right] &= \\text{E}\\left[\\frac{1}{\\mathcal{L}}\\frac{d\\mathcal{L}}{d\\theta}\\right] = \\int_\\Omega{\\left[\\frac{1}{\\mathcal{L}}\\frac{d\\mathcal{L}}{d\\theta}\\right]\\mathcal Ldz} \\\\\n",
    "&=\\frac{d}{d\\theta}\\int_\\Omega{\\mathcal{L}dz}=\\frac{d}{d\\theta}1=0 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "(C)\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{E}\\left[\\frac{d^2\\ln{\\mathcal{L}}}{d\\theta d\\theta'}\\right]&=\\text{E}\\left[\\frac{d}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]=\\text{E}\\left[\\frac{d}{d\\theta}\\left(\\frac{1}{\\mathcal{L}}\\frac{d\\mathcal{L}}{d\\theta'}\\right)\\right]\\\\\n",
    "&=\\text{E}\\left[\\left(\\frac{1}{\\mathcal{L}}\\frac{d^2\\mathcal{L}}{d\\theta d\\theta'}\\right)\\right]+\\text{E}\\left[\\left(\\frac{-1}{\\mathcal{L}^2}\\frac{d\\mathcal{L}}{d\\theta}\\frac{d\\mathcal{L}}{d\\theta'}\\right)\\right]\\\\\n",
    "&=\\int_\\Omega{\\frac{d^2\\mathcal{L}}{d\\theta d\\theta'}dz}-\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]\\\\\n",
    "&=\\frac{d^2}{d\\theta d\\theta'}\\int_\\Omega{\\mathcal{L}dz}-\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]\\\\\n",
    "&=\\mathbf{0}-\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]=-\\text{E}\\left[\\frac{d\\ln{\\mathcal{L}}}{d\\theta}\\frac{d\\ln{\\mathcal{L}}}{d\\theta'}\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "(D)\n",
    "$$\\int{\\frac{d\\mathcal{L}}{d\\theta}\\hat\\theta'dz}=\\frac{d}{d\\theta}\\int{\\mathcal{L}\\hat\\theta'dz}=\\frac{d}{d\\theta}E[\\hat\\theta]=\\frac{d}{d\\theta}\\theta=\\mathbf{I}_r$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Grid search\n",
    "\n",
    "Search over a given parameter space. Check every possible option for the optimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.004004004004004, 5.995995995995996)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "def grid_search(func,space,maximize=False):\n",
    "    vstates = [(x,func(x)) for x in space]\n",
    "    vstates.sort(key=lambda x: x[1])\n",
    "    if maximize: return vstates[-1][0]\n",
    "    return vstates[0][0]\n",
    "\n",
    "x = np.linspace(0,10,1000).tolist()\n",
    "func = lambda x: (x[0]-4.0001)**2*(x[1]-6.0001)**2\n",
    "grid_search(func,product(x,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient descent\n",
    "\n",
    "Walk along the slope of the curve by steps proportional to the opposite of the size of the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(func,gradient,init_x:np.ndarray,learning_rate:float=0.05,max_reps:int=1000,tolerance:float=1e-6,maximize=False):\n",
    "    x = init_x.copy()\n",
    "    for i in range(max_reps):\n",
    "        gx = gradient(x)\n",
    "        x0 = x.copy()\n",
    "        x += gx*learning_rate if maximize else -gx*learning_rate\n",
    "        if (func(x)<flast and maximize and i>2) or (func(x)>flast and (not maximize) and i>2): \n",
    "            x = x0\n",
    "            break\n",
    "    return x\n",
    "gradient_descent(gradient,np.array([0.75,0.15]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(gradient,hessian,init_x:np.ndarray,max_reps:int=100,tolerance:float=1e-6):\n",
    "    x = init_x.copy()\n",
    "    for i in range(max_reps):\n",
    "        update = -np.linalg.solve(hessian(x),gradient(x))\n",
    "        x += update\n",
    "        if np.abs(update).sum()<tolerance:\n",
    "            return (x,i)\n",
    "    raise Exception('Newton did not converge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleands import *\n",
    "\n",
    "class likelihood_model(learning_model):\n",
    "    def evaluate_lnL(self,pred): raise NotImplementedError\n",
    "    @property\n",
    "    def lnL(self): return self.evaluate_lnL(self.fitted)\n",
    "    @property\n",
    "    def aic(self): return 2*self.n_feat-2*self.lnL\n",
    "    @property\n",
    "    def bic(self): return np.log(self.n_obs)*self.n_feat-2*self.lnL\n",
    "    @property\n",
    "    def deviance(self): return 2*self.lnL-2*self._null_lnL_()\n",
    "    def _gradient_(self,coef): raise NotImplementedError\n",
    "    def _hessian_(self,coef): raise NotImplementedError\n",
    "    def _null_lnL_(self): return self.evaluate_lnL(np.ones(self.y.shape)*self.y.mean())\n",
    "    def __vcov_params_lnL__(self): return -np.linalg.inv(self._hessian_(self.params))\n",
    "    def __max_likelihood__(self,init_params,gradient=None,hessian=None):\n",
    "        if gradient==None: gradient=self._gradient_\n",
    "        if hessian==None: hessian=self._hessian_\n",
    "        return newton(gradient,hessian,init_params)\n",
    "class linear_model(prediction_model,likelihood_model):\n",
    "    def __init__(self,x,y):\n",
    "        super(linear_model,self).__init__(x,y)\n",
    "        self.params = self.__fit__(x,y)\n",
    "    def __fit__(self,x,y): return np.linalg.solve(x.T@x,x.T@y)\n",
    "    def predict(self,target): return target@self.params\n",
    "    def evaluate_lnL(self,pred): return -self.n_obs/2*(np.log(2*np.pi*(self.y-pred).var())+1)\n",
    "    @property\n",
    "    def r_squared(self):\n",
    "        return 1-self.residuals.var()/self.y.var()\n",
    "    @property\n",
    "    def adjusted_r_squared(self):\n",
    "        return 1-(1-self.r_squared)*(self.n_obs-1)/(self.n_obs-self.n_feat)\n",
    "    @property\n",
    "    def degrees_of_freedom(self):\n",
    "        return self.n_obs-self.n_feat\n",
    "    @property\n",
    "    def ssq(self):\n",
    "        return self.residuals.var()*(self.n_obs-1)/self.degrees_of_freedom\n",
    "class logistic_regressor(linear_model):\n",
    "    def __fit__(self,x,y):\n",
    "        params,self.iters = self.__max_likelihood__(np.zeros(self.n_feat))\n",
    "        return params\n",
    "    @property\n",
    "    def vcov_params(self):return self.__vcov_params_lnL__()\n",
    "    def evaluate_lnL(self,pred):return self.y.T@np.log(pred)+(1-self.y).T@np.log(1-pred)\n",
    "    def _gradient_(self,coefs):return self.x.T@(self.y-expit(self.x@coefs))\n",
    "    def _hessian_(self,coefs):\n",
    "        Fx = expit(self.x@coefs)\n",
    "        return -self.x.T@np.diagflat((Fx*(1-Fx)).values)@self.x\n",
    "    def predict(self,target):return expit(target@self.params)\n",
    "\n",
    "class LogisticRegressor(logistic_regressor,broom_model):\n",
    "    def __init__(self,x_vars:list,y_var:str,data:pd.DataFrame,*args,**kwargs):\n",
    "        super(LogisticRegressor,self).__init__(data[x_vars],data[y_var],*args,**kwargs)\n",
    "        self.x_vars = x_vars\n",
    "        self.y_var = y_var\n",
    "        self.data = data\n",
    "    def _glance_dict_(self):\n",
    "        return {'mcfadden.r.squared':self.r_squared,\n",
    "                'adjusted.r.squared':self.adjusted_r_squared,\n",
    "                'self.df':self.n_feat,\n",
    "                'resid.df':self.degrees_of_freedom,\n",
    "                'aic':self.aic,\n",
    "                'bic':self.bic,\n",
    "                'log.likelihood':self.lnL,\n",
    "                'deviance':self.deviance,\n",
    "                'resid.var':self.ssq}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Programming challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Recursive partitioning trees\n",
    "\n",
    "Write a class that implements a recursive partitioning algorithm. Use our common machine learning code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Quaternions\n",
    "\n",
    "The Quaternions are a generalization of complex numbers. Where the complex numbers have two components, $a$ and $b$, for a number $a+bi$, the Quaternions have four parts $a, b, c$ and $d$: $$a+bi+cj+dk$$\n",
    "\n",
    "The Quaternions have four basic operations: addition, subtraction, multiplication. and the inverse. Your job is to write a quaternion class which implements these operations. You can learn how to perform these operations on the Quaternions' wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
